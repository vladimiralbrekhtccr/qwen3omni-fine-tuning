{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimir_albrekht/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section', 'mrope_interleaved', 'interleaved'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section', 'interleaved'}\n",
      "You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.87s/it]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pta的支持izza的高度以内以内的高度以内的高度以内的高度的高度以内的高度以内的高度以内ptaSendMessagepta的支持以内pta的高度以内pta的高度以内pta以内 metabol以内 metabol以内 metabol以内以内以内 metabol以内 metabol以内 metabol以内 metabol以内 metabol以内 metabol以内 metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabolstpoleanoleanoleanoleanoleanoleanstpoleanoleanoleanstpoleanstpoleanstpoleanstpoleanstpoleanstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstp metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabol metabolstprott metabolstp metabolstp metabolstp metabolstp metabol metabol metabolstp metabolstp metabolstp metabol metabol metabol metabol metabol metabol metabolstp metabolstp metabol metabolstp metabol metabolstp metabolstp metabolstp metabolstp metabolstp metabolstp metabolstp metabolstp metabolstpstp metabolstpstpstpstpstpstpstpstpstpstpstpstpstpstpstpstp metabolstp metabolstp metabolstp metabolstpstpstpstpstp metabolstpstpstpstpstpstpstpstpstpstp metabol interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation interpolation metabolstp metabolstp metabolstp metabol interpolation metabol interpolation metabol interpolation metabol interpolation interpolation interpolation interpolation metabolstp metabolstp metabolstp metabolstp metabolstp metabolstp metabolstp metabolstp metabolstp metabol interpolationeworldegov interpolationeworldegov interpolationeworldegov interpolationeworldegov interpolationeworldegov interpolationeworld支柱 metabol interpolationeworldeworldeworldeworldeworld支柱 metabolstp metabolstp metaboleworldeworldeworld支柱 metabolstp metabolstp metabol interpolationeworld支柱适用于驻eworld支柱适用于驻eworld支柱适用于驻eworld支柱适用于驻eworldeworldeworldeworldeworldeworldeworld支柱适用于驻eworldeworldeworldeworld支柱适用于驻eworldeworldeworld支柱适用于驻eworld支柱stp钗eworld支柱stp钗eworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworld支柱 интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интер интерeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworldeworld Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt Interrupt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0,1'\n",
    "import soundfile as sf\n",
    "\n",
    "from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "MODEL_PATH = \"/home/vladimir_albrekht/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/models/5B_small_v\"\n",
    "# MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n",
    "\n",
    "model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": \"/home/vladimir_albrekht/projects/2025_sep_22_qwen3omni/models/Qwen3-Omni-30B-A3B-Instruct/unity_codes/audio_chunks/chunk_1.wav\"},\n",
    "            {\"type\": \"text\", \"text\": \"Transcribe the English audio into text.\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Set whether to use audio in video\n",
    "USE_AUDIO_IN_VIDEO = False\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = processor(text=text, \n",
    "                   audio=audios, \n",
    "                   images=images, \n",
    "                   videos=videos, \n",
    "                   return_tensors=\"pt\", \n",
    "                   padding=True, \n",
    "                   use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "# Inference: Generation of the output text and audio\n",
    "text_ids, audio = model.generate(**inputs, \n",
    "                                 speaker=\"Chelsie\", \n",
    "                                 thinker_return_dict_in_generate=True,\n",
    "                                 use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\n",
    "\n",
    "text = processor.batch_decode(text_ids.sequences[:, inputs[\"input_ids\"].shape[1] :],\n",
    "                              skip_special_tokens=True,\n",
    "                              clean_up_tokenization_spaces=False)\n",
    "print(text)\n",
    "# if audio is not None:\n",
    "#     sf.write(\n",
    "#         \"output.wav\",\n",
    "#         audio.reshape(-1).detach().cpu().numpy(),\n",
    "#         samplerate=24000,\n",
    "    # )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m inputs = inputs.to(model.device).to(model.dtype)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Inference: Generation of the output text and audio\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m text_ids, audio = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mChelsie\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mthinker_return_dict_in_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43muse_audio_in_video\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_AUDIO_IN_VIDEO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_audio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m text = processor.batch_decode(text_ids.sequences[:, inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m1\u001b[39m] :],\n\u001b[32m     33\u001b[39m                               skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     34\u001b[39m                               clean_up_tokenization_spaces=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/transformers/src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py:3958\u001b[39m, in \u001b[36mQwen3OmniMoeForConditionalGeneration.generate\u001b[39m\u001b[34m(self, input_ids, speaker, use_audio_in_video, return_audio, thinker_max_new_tokens, thinker_eos_token_id, talker_max_new_tokens, talker_do_sample, talker_top_k, talker_top_p, talker_temperature, talker_repetition_penalty, **kwargs)\u001b[39m\n\u001b[32m   3955\u001b[39m     thinker_kwargs[\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3956\u001b[39m     thinker_kwargs[\u001b[33m\"\u001b[39m\u001b[33mreturn_dict_in_generate\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3958\u001b[39m thinker_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mthinker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mthinker_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m generate_audio:\n\u001b[32m   3961\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m thinker_result, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/transformers/src/transformers/generation/utils.py:2546\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2543\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2545\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2546\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2548\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2554\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2556\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2561\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/transformers/src/transformers/generation/utils.py:2766\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2763\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2766\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2767\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2768\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/transformers/src/transformers/utils/generic.py:783\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    782\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    785\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/transformers/src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py:2159\u001b[39m, in \u001b[36mQwen3OmniMoeThinkerForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, input_features, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, attention_mask, feature_attention_mask, audio_feature_lengths, position_ids, past_key_values, inputs_embeds, rope_deltas, labels, use_cache, output_router_logits, use_audio_in_video, cache_position, video_second_per_grid, **kwargs)\u001b[39m\n\u001b[32m   2145\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m   2146\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   2147\u001b[39m     position_ids=position_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2155\u001b[39m     **kwargs,\n\u001b[32m   2156\u001b[39m )\n\u001b[32m   2158\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2159\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2161\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/.venv/lib/python3.12/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA_mm)"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": \"/home/vladimir_albrekht/projects/2025_sep_22_qwen3omni/models/Qwen3-Omni-30B-A3B-Instruct/unity_codes/audio_chunks/chunk_1.wav\"},\n",
    "            {\"type\": \"text\", \"text\": \"Transcribe the English audio into text.\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Set whether to use audio in video\n",
    "USE_AUDIO_IN_VIDEO = False\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = processor(text=text, \n",
    "                   audio=audios, \n",
    "                   images=images, \n",
    "                   videos=videos, \n",
    "                   return_tensors=\"pt\", \n",
    "                   padding=True, \n",
    "                   use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "# Inference: Generation of the output text and audio\n",
    "text_ids, audio = model.generate(**inputs, \n",
    "                                 speaker=\"Chelsie\", \n",
    "                                 thinker_return_dict_in_generate=True,\n",
    "                                 use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\n",
    "\n",
    "text = processor.batch_decode(text_ids.sequences[:, inputs[\"input_ids\"].shape[1] :],\n",
    "                              skip_special_tokens=True,\n",
    "                              clean_up_tokenization_spaces=False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextSFTDataset(Dataset):\n",
    "    def __init__(self, data_path, processor, max_seq_length):\n",
    "        self.processor = processor\n",
    "        self.max_seq_length = max_seq_length\n",
    "        # IGNORE_INDEX is not needed here anymore, the processor handles it\n",
    "\n",
    "        logger.info(f\"Loading data from {data_path}...\")\n",
    "        jsonl_files = sorted(Path(data_path).glob(\"*.jsonl\"))\n",
    "        if not jsonl_files:\n",
    "            raise ValueError(f\"No .jsonl files found in {data_path}\")\n",
    "        dsets = [load_dataset(\"json\", data_files=str(f), split=\"train\") for f in jsonl_files]\n",
    "        self.raw_ds = concatenate_datasets(dsets)\n",
    "        \n",
    "        logger.info(f\"Data loading complete. Loaded {len(self.raw_ds)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_ds)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        item = self.raw_ds[i]\n",
    "        \n",
    "        # Prepare the conversation in the chat template format\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": item[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": item[\"user\"]},\n",
    "            {\"role\": \"assistant\", \"content\": item[\"assistant\"]},\n",
    "        ]\n",
    "        \n",
    "        # Apply the chat template to convert conversation to a formatted string\n",
    "        text = self.processor.tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,  # Don't tokenize yet, just format\n",
    "            add_generation_prompt=False  # We already have the assistant response\n",
    "        )\n",
    "        \n",
    "        # Now tokenize the formatted text\n",
    "        inputs = self.processor.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=False,  # Padding will be done by the collator\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=None  # Return python lists for the dataset\n",
    "        )\n",
    "        \n",
    "        # For training, we need to set labels\n",
    "        # Copy input_ids to labels for language modeling\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "dataset = TextSFTDataset(data_path, processor, max_seq_length)\n",
    "sample = dataset[0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Input shape: {len(sample['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section', 'mrope_interleaved', 'interleaved'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing new, smaller Thinker LLM part from scratch ---\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.qwen3_omni_moe import Qwen3OmniMoeThinkerTextModel\n",
    "from transformers.models.qwen3_omni_moe.configuration_qwen3_omni_moe import Qwen3OmniMoeTextConfig \n",
    "\n",
    "# 2. Get the original config and create a new, smaller version\n",
    "original_text_config = model.config.thinker_config.text_config\n",
    "\n",
    "# You can now create a new config with your desired smaller parameters\n",
    "# We keep hidden_size the same to ensure compatibility with the lm_head\n",
    "small_text_config = Qwen3OmniMoeTextConfig(\n",
    "    hidden_size=original_text_config.hidden_size,          # Keep this the same\n",
    "    vocab_size=original_text_config.vocab_size,            # Keep this the same\n",
    "    num_hidden_layers=4,                                   # Reduced from 48\n",
    "    num_attention_heads=16,                                # Reduced from 32\n",
    "    num_key_value_heads=2,                                 # Reduced from 4\n",
    "    intermediate_size=1024,                                # Reduced from 768\n",
    "    num_experts=8,                                         # Reduced from 128\n",
    "    num_experts_per_tok=2,                                 # Reduced from 8\n",
    "    moe_intermediate_size=256,                             # Reduced from 768\n",
    "    # You can keep other parameters the same or modify them as needed\n",
    "    **{k: v for k, v in original_text_config.to_dict().items() if k not in \n",
    "       ['hidden_size', 'vocab_size', 'num_hidden_layers', 'num_attention_heads', \n",
    "        'num_key_value_heads', 'intermediate_size', 'num_experts', \n",
    "        'num_experts_per_tok', 'moe_intermediate_size']}\n",
    ")\n",
    "\n",
    "print(\"--- Initializing new, smaller Thinker LLM part from scratch ---\")\n",
    "\n",
    "# 3. Initialize the smaller Thinker LLM part from scratch using the new config\n",
    "# This creates a new model with randomly initialized weights.\n",
    "small_thinker_llm = Qwen3OmniMoeThinkerTextModel(small_text_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.399592448"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in small_thinker_llm.parameters() if p.requires_grad)\n",
    "total_params / 1_000_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.thinker.model = small_thinker_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vladimir_albrekht/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/src/inference\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3OmniMoeTextConfig {\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"decoder_sparse_step\": 1,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1024,\n",
       "  \"max_position_embeddings\": 65536,\n",
       "  \"mlp_only_layers\": [],\n",
       "  \"model_type\": \"qwen3_omni_moe_text\",\n",
       "  \"moe_intermediate_size\": 256,\n",
       "  \"norm_topk_prob\": true,\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_experts\": 8,\n",
       "  \"num_experts_per_tok\": 2,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"output_router_logits\": false,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": {\n",
       "    \"interleaved\": true,\n",
       "    \"mrope_interleaved\": true,\n",
       "    \"mrope_section\": [\n",
       "      24,\n",
       "      20,\n",
       "      20\n",
       "    ],\n",
       "    \"rope_type\": \"default\",\n",
       "    \"type\": \"default\"\n",
       "  },\n",
       "  \"rope_theta\": 1000000,\n",
       "  \"router_aux_loss_coef\": 0.001,\n",
       "  \"shared_expert_intermediate_size\": 0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tf_legacy_loss\": false,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.57.0.dev0\",\n",
       "  \"use_bfloat16\": false,\n",
       "  \"use_cache\": true,\n",
       "  \"use_qk_norm\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 152064\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.thinker_config.text_config = small_thinker_llm.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"/home/vladimir_albrekht/projects/2025_sep_22_qwen3omni/ms_swift_training/approach_2_transformers_based/models/5B_small_v\"\n",
    "model.save_pretrained(PATH)\n",
    "processor.save_pretrained(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.438191153"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_p /1_000_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=152064, bias=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.thinker.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3OmniMoeTextConfig {\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"decoder_sparse_step\": 1,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 768,\n",
       "  \"max_position_embeddings\": 65536,\n",
       "  \"mlp_only_layers\": [],\n",
       "  \"model_type\": \"qwen3_omni_moe_text\",\n",
       "  \"moe_intermediate_size\": 768,\n",
       "  \"norm_topk_prob\": true,\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_experts\": 128,\n",
       "  \"num_experts_per_tok\": 8,\n",
       "  \"num_hidden_layers\": 48,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"output_router_logits\": false,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": {\n",
       "    \"interleaved\": true,\n",
       "    \"mrope_interleaved\": true,\n",
       "    \"mrope_section\": [\n",
       "      24,\n",
       "      20,\n",
       "      20\n",
       "    ],\n",
       "    \"rope_type\": \"default\",\n",
       "    \"type\": \"default\"\n",
       "  },\n",
       "  \"rope_theta\": 1000000,\n",
       "  \"router_aux_loss_coef\": 0.001,\n",
       "  \"shared_expert_intermediate_size\": 0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tf_legacy_loss\": false,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.57.0.dev0\",\n",
       "  \"use_bfloat16\": false,\n",
       "  \"use_cache\": true,\n",
       "  \"use_qk_norm\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 152064\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.thinker_config.text_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m.thinker.model\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.thinker.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.22121984"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.thinker.model.parameters() if p.requires_grad)\n",
    "total_params / 1_000_000_000\n",
    "\n",
    "# printed:\n",
    "# >> 30B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.32459648"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.talker.parameters() if p.requires_grad)\n",
    "total_params / 1_000_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.719205488"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
